{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Project\"\n",
        "author: \"Mario Venegas, Nasser Alshaya and Daniel Avila\"\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "execute:\n",
        "  echo: true     \n",
        "  warning: false  \n",
        "  message: false  \n",
        "  error: false \n",
        "---\n",
        "\n",
        "\n",
        "Note: some chuncks were commeneted out because of the size of the files which cannot be pushed to the repo, all files are included in a Drive link\n",
        "\n",
        "**[Drive](https://drive.google.com/drive/u/0/folders/1q9t4VtaZ1b81MvYmoRcrrbEFFR_3E8qz)**\n",
        "\n",
        "## CTA Routes Data\n",
        "\n",
        "1.\n"
      ],
      "id": "d970a187"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import altair as alt\n",
        "\n",
        "bus_routes = pd.read_csv('data/cta_br.csv')\n",
        "\n",
        "# Restrict the data to routes 171 and 172\n",
        "bus_routes = bus_routes[bus_routes['route'].isin(['171', '172'])].copy()\n",
        "\n",
        "# From 2018 onwards\n",
        "bus_routes['date'] = pd.to_datetime(bus_routes['date'])\n",
        "\n",
        "bus_routes = bus_routes[bus_routes['date'] >= '2018-01-01']\n",
        "\n",
        "# Display the first few rows\n",
        "bus_routes.head()\n",
        "\n",
        "# Number of of observations\n",
        "row_count = bus_routes.shape[0]\n",
        "print(f\"Number of rows: {row_count}\")"
      ],
      "id": "d4bebee6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "0b6689f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "# Extract year and month\n",
        "bus_routes['year'] = bus_routes['date'].dt.year\n",
        "bus_routes['month'] = bus_routes['date'].dt.month\n",
        "\n",
        "# Group by year and month, and count rows\n",
        "cta_grouped = bus_routes.groupby(['route', 'date'])['rides'].sum().reset_index(name='rides') \n",
        "\n",
        "# The data is uploaded daily for each of the routes, this is why the count is twice as much as the days in each month. \n",
        "\n",
        "# Sort the filtered data by the 'date' column\n",
        "bus_routes = bus_routes.sort_values(by='date')"
      ],
      "id": "95500a62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n"
      ],
      "id": "4fe6d4cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "# Taking the above code block, finding the average for each month, from 2018 to 2024, and plotting this.\n",
        "\n",
        "months = {\n",
        "    1: \"January\", \n",
        "    2: \"February\", \n",
        "    3: \"March\",\n",
        "    4: \"April\",\n",
        "    5: \"May\",\n",
        "    6: \"June\",\n",
        "    7: \"July\",\n",
        "    8: \"August\",\n",
        "    9: \"September\",\n",
        "    10: \"October\", \n",
        "    11: \"November\",\n",
        "    12: \"December\"}\n",
        "\n",
        "cta_grouped[\"month\"] = cta_grouped[\"date\"].dt.month\n",
        "cta_grouped[\"year\"] = cta_grouped[\"date\"].dt.year\n",
        "\n",
        "cta_grouped = cta_grouped.groupby([\"month\", \"year\"])[\"rides\"].sum().reset_index(name = \"rides\")\n",
        "cta_grouped[\"month\"] = cta_grouped[\"month\"].apply(lambda x: months[x])\n",
        "\n",
        "\n",
        "average_ridership_list = []\n",
        "months_list = []\n",
        "for month in cta_grouped[\"month\"].unique():\n",
        "    average_ridership = cta_grouped[cta_grouped[\"month\"] == month]\n",
        "    average_ridership = average_ridership[\"rides\"].mean()\n",
        "    average_ridership_list.append(round(average_ridership, 2))\n",
        "    months_list.append(month)\n",
        "\n",
        "average_ridership_df = pd.DataFrame({\"monthly_avg_riders\": average_ridership_list,\n",
        "                                     \"month\": months_list})\n",
        "\n",
        "\n",
        "# making the bar graph\n",
        "bar_chart = alt.Chart(average_ridership_df).mark_bar().encode(\n",
        "    x=alt.X('month:N', sort=['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
        "    'August', 'September', 'October', 'November', 'December']),  \n",
        "    y='monthly_avg_riders:Q',\n",
        ").properties(\n",
        "    width=800, \n",
        "    height=400  \n",
        ")\n",
        "\n",
        "\n",
        "# heatmap of month on x axis and year on y axis\n",
        "heatmap = alt.Chart(cta_grouped).mark_rect().encode(\n",
        "    x=alt.X('month:N', title='Month'),\n",
        "    y=alt.Y('year:O', title='Year'), \n",
        "    color = alt.Color(\"rides\")\n",
        ").properties(\n",
        "    width=800,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "bar_chart | heatmap"
      ],
      "id": "028bab52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Divvy ridership\n",
        "\n",
        "**This chunck was commented out and all data can be found on Drive**"
      ],
      "id": "9f62a82b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "\n",
        "'''\n",
        "# Up to 2020, the information was uploaded quarterly, after this, it started to be uploaded monthly. Before merging the data, it is necessary to explore the data bases. All these data sets are included in the divy_data folder in the Drive\n",
        "\n",
        "# Quarterly data base:\n",
        "quarterly_sample = pd.read_csv('divvy_data/Divvy_Trips_2020_Q1.csv')\n",
        "\n",
        "print(quarterly_sample.head())\n",
        "\n",
        "# Monthly data base:\n",
        "monthly_sample = pd.read_csv('divvy_data/202004-divvy-tripdata.csv')\n",
        "\n",
        "print(monthly_sample.head())\n",
        "'''"
      ],
      "id": "57c496f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Divvy and Spatial Data\n",
        "5. \n"
      ],
      "id": "672a6cec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import shape, Point\n",
        "\n",
        "# Data base containing geometry information about Chicago\n",
        "\n",
        "chicago_areas = pd.read_csv('data/chicago_areas.csv') \n",
        "\n",
        "# Data base containing geographic information of Divvy docking stations\n",
        "stations_gdf = gpd.read_file('Divvy stations/divvy_stations.shp') \n",
        "\n",
        "# Ensure the shapefile has a valid CRS\n",
        "stations_gdf = stations_gdf.to_crs(\"EPSG:4326\") \n",
        "\n",
        "# Convert the geometry column in chicago_areas to shapely geometry to make it compatible with the station_gdf df. \n",
        "\n",
        "from shapely.wkt import loads\n",
        "\n",
        "# Convert the 'geometry' column to actual shapely geometry\n",
        "chicago_areas['the_geom'] = chicago_areas['the_geom'].apply(loads)\n",
        "\n",
        "# Convert to a GeoDataFrame\n",
        "chicago_areas = gpd.GeoDataFrame(chicago_areas, geometry='the_geom', crs=\"EPSG:4326\")\n",
        "\n",
        "# Limit geographic location to Hyde Park\n",
        "\n",
        "hyde_park = chicago_areas[chicago_areas['COMMUNITY'] == 'HYDE PARK']\n",
        "\n",
        "# Perform spatial joint to print the name of the Divvy Stations in Hyde Park\n",
        "\n",
        "# Ensure both GeoDataFrames have the same CRS\n",
        "stations_gdf = stations_gdf.to_crs(hyde_park.crs)\n",
        "\n",
        "# Perform spatial join\n",
        "stations_in_hp = gpd.sjoin(stations_gdf, hyde_park, predicate='within')\n",
        "\n",
        "# Print the name of the bike stations\n",
        "unique_stations = stations_in_hp['station_na'].unique()"
      ],
      "id": "6e08b25d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. \n",
        "\n",
        "6.1.\n",
        "\n",
        "**This chunk was commented out to speed up the knitting process, large files found on Drive**"
      ],
      "id": "78305ef3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "'''\n",
        "#Monthly_data\n",
        "\n",
        "# Name the folder that contains all the quarterly and monthly information of divvy trips\n",
        "folder_path = 'divvy_data'\n",
        "\n",
        "# List of unique stations in Hyde Park\n",
        "unique_stations = ['Cornell Ave & Hyde Park Blvd', 'Griffin Museum of Science and Industry', \n",
        "'Blackstone Ave & 59th St' 'Ellis Ave & 58th St', 'University Ave & 59th St','University Ave & 57th St', \n",
        "'Ellis Ave & 53rd St','Lake Park Ave & 53rd St','Lake Park Ave & 56th St', 'Ellis Ave & 55th St',\n",
        "'Harper Ave & 59th St','Woodlawn Ave & 55th St', 'Kimbark Ave & 53rd St','Woodlawn Ave & 58th St',\n",
        "'Shore Dr & 55th St']\n",
        "\n",
        "# List to hold the filtered DataFrames\n",
        "filtered_data = []\n",
        "\n",
        "# Loop through all files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    # Ensure only CSV files are processed\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        \n",
        "        # Read the current CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Check if 'start_station_name' and 'end_station_name' columns exist\n",
        "        if {'start_station_name', 'end_station_name'}.issubset(df.columns):\n",
        "            # Filter rows where both start and end stations are in unique_stations\n",
        "            filtered_df = df[\n",
        "                df['start_station_name'].isin(unique_stations) & \n",
        "                df['end_station_name'].isin(unique_stations)\n",
        "            ]\n",
        "            \n",
        "            # Append the filtered DataFrame to the list\n",
        "            filtered_data.append(filtered_df)\n",
        "\n",
        "# Combine all filtered DataFrames into one\n",
        "if filtered_data:\n",
        "    monthly_divvy_df = pd.concat(filtered_data, ignore_index=True)\n",
        "    print(\"All files have been processed and merged.\")\n",
        "    \n",
        "    # Save the final merged DataFrame to a new CSV\n",
        "    monthly_divvy_df.to_csv('Monthly_Bike_Trips.csv', index=False)\n",
        "    print(\"Filtered data saved to 'Monthly_Bike_Trips.csv'.\")\n",
        "else:\n",
        "    print(\"No data was filtered or no files were processed.\")\n",
        "'''"
      ],
      "id": "93e599cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.2.\n",
        "\n",
        "**This chunk was commented out to speed up the knitting process, large files found on Drive**"
      ],
      "id": "3bffb67f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "'''\n",
        "#Quarterly\n",
        "# Name the folder that contains all the quarterly and monthly information of divvy trips\n",
        "folder_path = 'divvy_data'\n",
        "\n",
        "# List of unique stations in Hyde Park\n",
        "unique_stations = ['Cornell Ave & Hyde Park Blvd', 'Griffin Museum of Science and Industry', \n",
        "'Blackstone Ave & 59th St', 'Ellis Ave & 58th St', 'University Ave & 59th St','University Ave & 57th St', \n",
        "'Ellis Ave & 53rd St','Lake Park Ave & 53rd St','Lake Park Ave & 56th St', 'Ellis Ave & 55th St',\n",
        "'Harper Ave & 59th St','Woodlawn Ave & 55th St', 'Kimbark Ave & 53rd St','Woodlawn Ave & 58th St',\n",
        "'Shore Dr & 55th St']\n",
        "\n",
        "# List to hold the filtered DataFrames\n",
        "filtered_data = []\n",
        "\n",
        "# Loop through all files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    # Ensure only CSV files are processed\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        \n",
        "        # Read the current CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Check if 'from_station_name' and 'to_station_name' columns exist\n",
        "        if {'from_station_name', 'to_station_name'}.issubset(df.columns):\n",
        "            # Filter rows where both start and end stations are in unique_stations\n",
        "            filtered_df = df[\n",
        "                df['from_station_name'].isin(unique_stations) & \n",
        "                df['to_station_name'].isin(unique_stations)\n",
        "            ]\n",
        "            \n",
        "            # Append the filtered DataFrame to the list\n",
        "            filtered_data.append(filtered_df)\n",
        "\n",
        "# Combine all filtered DataFrames into one\n",
        "if filtered_data:\n",
        "    divvy_df = pd.concat(filtered_data, ignore_index=True)\n",
        "    print(\"All files have been processed and merged.\")\n",
        "    \n",
        "    # Save the final merged DataFrame to a new CSV\n",
        "    divvy_df.to_csv('Quarterly_Bike_Trips.csv', index=False)\n",
        "    print(\"Filtered data saved to 'Quarterly_Bike_Trips.csv'.\")\n",
        "else:\n",
        "    print(\"No data was filtered or no files were processed.\")\n",
        "'''"
      ],
      "id": "237012f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.3.\n",
        "\n",
        "**This chunk was commented out to speed up the knitting process, large files found on Drive**"
      ],
      "id": "e3abea25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "'''\n",
        "#Quarterly 2. Two of the data sets have different column names\n",
        "\n",
        "# Name the folder that contains all the quarterly and monthly information of divvy trips\n",
        "folder_path = 'divvy_data'\n",
        "\n",
        "# List of unique stations in Hyde Park\n",
        "unique_stations = ['Cornell Ave & Hyde Park Blvd', 'Griffin Museum of Science and Industry', \n",
        "'Blackstone Ave & 59th St', 'Ellis Ave & 58th St', 'University Ave & 59th St','University Ave & 57th St', \n",
        "'Ellis Ave & 53rd St','Lake Park Ave & 53rd St','Lake Park Ave & 56th St', 'Ellis Ave & 55th St',\n",
        "'Harper Ave & 59th St','Woodlawn Ave & 55th St', 'Kimbark Ave & 53rd St','Woodlawn Ave & 58th St',\n",
        "'Shore Dr & 55th St']\n",
        "\n",
        "# List to hold the filtered DataFrames\n",
        "filtered_data = []\n",
        "\n",
        "# Loop through all files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    # Ensure only CSV files are processed\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        \n",
        "        # Read the current CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # Check if '03 - Rental Start Station Name' and '02 - Rental End Station Name' columns exist\n",
        "        if {'03 - Rental Start Station Name', '02 - Rental End Station Name'}.issubset(df.columns):\n",
        "            # Filter rows where both start and end stations are in unique_stations\n",
        "            filtered_df = df[\n",
        "                df['03 - Rental Start Station Name'].isin(unique_stations) & \n",
        "                df['02 - Rental End Station Name'].isin(unique_stations)\n",
        "            ]\n",
        "            \n",
        "            # Append the filtered DataFrame to the list\n",
        "            filtered_data.append(filtered_df)\n",
        "\n",
        "# Combine all filtered DataFrames into one\n",
        "if filtered_data:\n",
        "    divvy_df = pd.concat(filtered_data, ignore_index=True)\n",
        "    print(\"All files have been processed and merged.\")\n",
        "    \n",
        "    # Save the final merged DataFrame to a new CSV\n",
        "    divvy_df.to_csv('Quarterly_Bike_Trips_2.csv', index=False)\n",
        "    print(\"Filtered data saved to 'Quarterly_Bike_Trips_2.csv'.\")\n",
        "else:\n",
        "    print(\"No data was filtered or no files were processed.\")\n",
        "\n",
        "'''"
      ],
      "id": "5bb1c8e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. \n"
      ],
      "id": "730d6583"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This code cell is just naming the dataframes so that we can merge them later.\n",
        "# In the Monthly data set, we care about the columns: 'started_at', 'ended_at', 'start_station_name', 'end_station_name'\n",
        "monthly_rides_dv = pd.read_csv('data/Monthly_Bike_Trips.csv')\n",
        "\n",
        "# In the Quarterly 1 data set, we care about the columns: 'start_time', 'end_time', 'from_station_name', 'to_station_name'\n",
        "quarterly_rides_dv = pd.read_csv('data/Quarterly_Bike_Trips.csv')\n",
        "\n",
        "# In the Quarterly 2 data set, we care about the columns: '01 - Rental Details Local Start Time', '01 - Rental Details Local End Time','03 - Rental Start Station Name', '02 - Rental End Station Name'\n",
        "quarterly_rides_dv_2 = pd.read_csv('data/Quarterly_Bike_Trips_2.csv')"
      ],
      "id": "6c351f48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. "
      ],
      "id": "0bcd7a05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# merging the quarterly data into one concat'd dataset\n",
        "\n",
        "#creating dictionaries to prepare for renaming columns in relevant datasets.\n",
        "quarterly_rides_dv_2 = quarterly_rides_dv_2.rename(columns = {\n",
        "        \"01 - Rental Details Local Start Time\" : \"start_time\",\n",
        "        \"01 - Rental Details Local End Time\" : \"end_time\",\n",
        "        \"03 - Rental Start Station Name\" : \"from_station_name\",\n",
        "        \"02 - Rental End Station Name\" : \"to_station_name\"\n",
        "})\n",
        "\n",
        "monthly_rides_dv = monthly_rides_dv.rename(columns = {\n",
        "    \"started_at\" : \"start_time\",\n",
        "    \"ended_at\" : \"end_time\",\n",
        "    \"start_station_name\" : \"from_station_name\",\n",
        "    \"end_station_name\" : \"to_station_name\"\n",
        "})\n",
        "\n",
        "#matching all of the names of the datasets.\n",
        "quarterly_rides_dv_2 = quarterly_rides_dv_2[[\"start_time\", \"end_time\", \"from_station_name\", \"to_station_name\"]]\n",
        "quarterly_rides_dv = quarterly_rides_dv[[\"start_time\", \"end_time\", \"from_station_name\", \"to_station_name\"]]\n",
        "monthly_rides_dv = monthly_rides_dv[[\"start_time\", \"end_time\", \"from_station_name\", \"to_station_name\"]]\n",
        "\n",
        "#creating one large dataset\n",
        "quarterly_rides = pd.concat([quarterly_rides_dv, quarterly_rides_dv_2, monthly_rides_dv], axis = 0)\n",
        "\n",
        "#verifying that data is in correct format.\n",
        "quarterly_rides['start_time'] = pd.to_datetime(quarterly_rides[\"start_time\"], format='mixed')\n",
        "quarterly_rides[\"end_time\"] = pd.to_datetime(quarterly_rides[\"end_time\"], format='mixed')\n",
        "quarterly_rides['year_month'] = quarterly_rides['start_time'].dt.strftime('%Y-%m')\n",
        "\n",
        "# Group by station and the year_month, and count the rides for each group\n",
        "rides_per_station_per_month = quarterly_rides.groupby(['from_station_name', 'year_month']).size().reset_index(name='ride_count')\n",
        "\n",
        "# Show the result\n",
        "rides_per_station_per_month.head()"
      ],
      "id": "d29a4410",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "9. "
      ],
      "id": "52c01023"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# creating a year column\n",
        "rides_per_station_per_month[\"year\"] = pd.to_datetime(rides_per_station_per_month['year_month']).dt.year\n",
        "\n",
        "#filtering for pre and post covid, for the purpose of visualizations\n",
        "rides_per_station_per_month_2017 = rides_per_station_per_month[rides_per_station_per_month[\"year\"] < 2020]\n",
        "rides_per_station_per_month_2020 = rides_per_station_per_month[rides_per_station_per_month[\"year\"] >= 2020]\n",
        "\n",
        "lasagna2017 =  alt.Chart(rides_per_station_per_month_2017).mark_rect().encode(\n",
        "    x=alt.X('year_month:N', title='Year-Month', sort=None), \n",
        "    y=alt.Y('from_station_name:N', title='Station Name'), \n",
        "    color=alt.Color('ride_count:Q', scale=alt.Scale(scheme='blues'), title='Ride Count'),  \n",
        ").properties(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    title=\"Monthly Divvy Rides per Station pre Covid, Hyde Park\"\n",
        ")\n",
        "\n",
        "lasagna2020 =  alt.Chart(rides_per_station_per_month_2020).mark_rect().encode(\n",
        "    x=alt.X('year_month:N', title='Year-Month', sort=None), \n",
        "    y=alt.Y('from_station_name:N', title='Station Name'),\n",
        "    color=alt.Color('ride_count:Q', scale=alt.Scale(scheme='blues'), title='Ride Count'), \n",
        ").properties(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    title=\"Monthly Divvy Rides per Station post Covid, Hyde Park\"\n",
        ")"
      ],
      "id": "6ab569ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lasagna2017 | lasagna2020"
      ],
      "id": "51032439",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10. "
      ],
      "id": "cf83741d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate average rides per month\n",
        "avg_rides_per_month = rides_per_station_per_month.groupby('year_month')['ride_count'].mean().reset_index(name='avg_ride_count')\n",
        "\n",
        "# Extract year and month to filter data by full years\n",
        "avg_rides_per_month[\"year_month\"] = pd.to_datetime(avg_rides_per_month[\"year_month\"])\n",
        "avg_rides_per_month['year'] = avg_rides_per_month['year_month'].dt.year\n",
        "avg_rides_per_month['month'] = avg_rides_per_month['year_month'].dt.month\n",
        "\n",
        "months = {\n",
        "    1: \"January\", \n",
        "    2: \"February\", \n",
        "    3: \"March\",\n",
        "    4: \"April\",\n",
        "    5: \"May\",\n",
        "    6: \"June\",\n",
        "    7: \"July\",\n",
        "    8: \"August\",\n",
        "    9: \"September\",\n",
        "    10: \"October\", \n",
        "    11: \"November\",\n",
        "    12: \"December\"}\n",
        "\n",
        "avg_rides_per_month_filtered = avg_rides_per_month\n",
        "avg_rides_per_month_filtered[\"month\"] = avg_rides_per_month[\"month\"].apply(lambda x: months[x]).astype(str)\n",
        "\n",
        "avg_rides_per_month_filtered_groupby = avg_rides_per_month_filtered.groupby(\"month\").mean().reset_index()\n",
        "\n",
        "# making the bar graph\n",
        "bar_chart_divvy = alt.Chart(avg_rides_per_month_filtered_groupby).mark_bar().encode(\n",
        "    x=alt.X('month:N', sort=['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
        "    'August', 'September', 'October', 'November', 'December']),\n",
        "    y='avg_ride_count:Q',  \n",
        ").properties(\n",
        "    width=800, \n",
        "    height=400,\n",
        "    title= \"Average Divvy Monthly Ridership, Hyde Park\"  \n",
        ")\n",
        "\n",
        "bar_chart_divvy"
      ],
      "id": "5490287a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11."
      ],
      "id": "fcbfdaa1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#creating the file that nasser needs for the dashboard\n",
        "quarterly_rides[\"year_month\"] = pd.to_datetime(quarterly_rides[\"year_month\"])\n",
        "quarterly_rides[\"month\"] = quarterly_rides[\"year_month\"].dt.month\n",
        "\n",
        "quarterly_rides_groupby = quarterly_rides.groupby(\"month\").count().reset_index().drop([\"end_time\", \"from_station_name\", \"to_station_name\", \"year_month\"], axis = 1).rename(columns = {\"start_time\" : \"count\"})"
      ],
      "id": "4280d60c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "c:\\Users\\mario\\OneDrive\\Desktop\\Repo_P2\\Final project\\final_project\\.venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}